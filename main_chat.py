import streamlit as st
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from openai import OpenAI
import io
from io import StringIO
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI


# # RAG êµ¬ì„± í•¨ìˆ˜
# def create_rag(files):
#     documents = []
#     for file in files:
#         if file.type == "application/pdf":
#             loader = PyPDFLoader(file.path)
#         elif file.type == "text/plain":
#             content = file.read().decode("utf-8")
#             loader = TextLoader(io.StringIO(content))
#         else:
#             st.error("Unsupported file type.")
#             return None
        
#         docs = loader.load()
#         print(len(docs))
#         # documents.extend(loader.load())
#         # for doc in loader.lazy_load():
#         #     print(doc.metadata)
#         #     documents.extend(doc)
#     embeddings = OpenAIEmbeddings()
#     vector_store = FAISS.from_documents(documents, embeddings)
#     retriever = vector_store.as_retriever()
#     return RetrievalQA(llm=OpenAI(), retriever=retriever)

def pdf_read(pdf_doc):
    text = ""
    for pdf in pdf_doc:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

def get_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_text(text)
    return chunks

def vector_store(text_chunks):
    embeddings = OpenAIEmbeddings()
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_db")

# RAG êµ¬ì„± í•¨ìˆ˜
def create_rag(files):

    raw_text = pdf_read(files)
    text_chunks = get_chunks(raw_text)
    embeddings = OpenAIEmbeddings()
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    retriever=vector_store.as_retriever()

    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context) ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ `ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤` ë¼ê³  ë‹µí•˜ì„¸ìš”.
    í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.

    #Question:
    {question}

    #Context:
    {context}

    #Answer:"""
    )

    llm = ChatOpenAI(model_name=st.session_state.selected_model, temperature=0)
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain

# ë©”ì¸ ì±„íŒ… í™”ë©´ êµ¬ì„±
def main_chat():
    # st.sidebar.write("\n".join([f"Conversation {i+1}: {msg['content'][:20]}..." for i, msg in enumerate(st.session_state['messages'][-10:])]))
    selected_model = st.sidebar.selectbox("Model", ["gpt-4o-mini", "gpt-4o"])
    if "selected_model" not in st.session_state:
        st.session_state.selected_model = selected_model
    temperature = st.sidebar.slider("Temperature", min_value=0., max_value=1., step=0.1)
    # ì±„íŒ… ëŒ€í™” ê¸°ë¡ ì €ì¥
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
    # st.title("Chat with GPT")
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            with st.chat_message('user', avatar="ğŸ‘©â€ğŸ“"):
                st.write(msg['content'])
        else:
            with st.chat_message('assistant', avatar="ğŸ§ "):
                st.write(msg['content'])

    # íŒŒì¼ ì—…ë¡œë“œ ë° RAG êµ¬ì„±
    uploaded_files = st.sidebar.file_uploader("Upload PDF or TXT files", accept_multiple_files=True, type=["pdf", "txt"])
    if uploaded_files:
        rag_chain = create_rag(uploaded_files)
    else:
        rag_chain = None

    user_input = st.chat_input("Message GPT")

    if user_input:
        with st.spinner("Thinking..."):
            llm = OpenAI()
            st.session_state.messages.append({"role": "user", "content": user_input})
            
            if rag_chain:
                assistant_message = rag_chain.invoke(user_input)
            else:
                answer = llm.chat.completions.create(
                    model=selected_model,
                    messages=st.session_state.messages,
                    temperature=temperature,
                )
                assistant_message = answer.choices[0].message.content
            st.session_state.messages.append({"role": "assistant", "content": assistant_message})

            with st.chat_message('user', avatar="ğŸ‘©â€ğŸ“"):
                st.write(user_input)
            with st.chat_message('assistant', avatar="ğŸ§ "):
                st.write(assistant_message)
